{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bb449wzsnKRW",
        "outputId": "dd58711c-e8bf-44de-ad0b-68332d205bc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment ready ✅\n",
            "Torch version: 2.9.0+cpu\n",
            "Using device: cpu\n",
            "Captum available ✅\n"
          ]
        }
      ],
      "source": [
        "# ============================\n",
        "# CELL 1: Environment Setup\n",
        "# ============================\n",
        "\n",
        "# Core\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# ML\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Explainability\n",
        "from captum.attr import IntegratedGradients\n",
        "\n",
        "# ----------------------------\n",
        "# Reproducibility\n",
        "# ----------------------------\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# ----------------------------\n",
        "# Device\n",
        "# ----------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(\"Environment ready ✅\")\n",
        "print(\"Torch version:\", torch.__version__)\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Verify Captum\n",
        "try:\n",
        "    _ = IntegratedGradients\n",
        "    print(\"Captum available ✅\")\n",
        "except:\n",
        "    print(\"Captum NOT available ❌\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0JPEYaUnTjv",
        "outputId": "319ebdf7-0925-4b48-9bf5-c4b7a0a0265b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: captum in /usr/local/lib/python3.12/dist-packages (0.8.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from captum) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.0 in /usr/local/lib/python3.12/dist-packages (from captum) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from captum) (25.0)\n",
            "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.12/dist-packages (from captum) (2.9.0+cpu)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from captum) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->captum) (3.20.2)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->captum) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->captum) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->captum) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->captum) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->captum) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->captum) (2025.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->captum) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->captum) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->captum) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->captum) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->captum) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->captum) (3.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->captum) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->captum) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.10->captum) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.10->captum) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install captum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 994
        },
        "id": "_ER__Vi6naxn",
        "outputId": "3ed8bb21-f81f-4265-c232-81676b7ad060"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3502246302.py:10: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  train_df = pd.read_csv(TRAIN_PATH)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train shape: (1017209, 9)\n",
            "Test shape : (41088, 8)\n",
            "Store shape: (1115, 10)\n",
            "\n",
            "--- Train columns ---\n",
            "['Store', 'DayOfWeek', 'Date', 'Sales', 'Customers', 'Open', 'Promo', 'StateHoliday', 'SchoolHoliday']\n",
            "\n",
            "--- Sample rows (train) ---\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"print(train_merged['Sales']\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Store\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2,\n          5,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"DayOfWeek\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 5,\n        \"max\": 5,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Date\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"2015-07-31\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sales\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3771,\n        \"min\": 4822,\n        \"max\": 13995,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          6064\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Customers\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 398,\n        \"min\": 555,\n        \"max\": 1498,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          625\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Open\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Promo\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"StateHoliday\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"0\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SchoolHoliday\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-20ee27d0-3a09-46fa-b107-47565e26b7a3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Store</th>\n",
              "      <th>DayOfWeek</th>\n",
              "      <th>Date</th>\n",
              "      <th>Sales</th>\n",
              "      <th>Customers</th>\n",
              "      <th>Open</th>\n",
              "      <th>Promo</th>\n",
              "      <th>StateHoliday</th>\n",
              "      <th>SchoolHoliday</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2015-07-31</td>\n",
              "      <td>5263</td>\n",
              "      <td>555</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2015-07-31</td>\n",
              "      <td>6064</td>\n",
              "      <td>625</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>2015-07-31</td>\n",
              "      <td>8314</td>\n",
              "      <td>821</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>2015-07-31</td>\n",
              "      <td>13995</td>\n",
              "      <td>1498</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2015-07-31</td>\n",
              "      <td>4822</td>\n",
              "      <td>559</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-20ee27d0-3a09-46fa-b107-47565e26b7a3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-20ee27d0-3a09-46fa-b107-47565e26b7a3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-20ee27d0-3a09-46fa-b107-47565e26b7a3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "   Store  DayOfWeek        Date  Sales  Customers  Open  Promo StateHoliday  \\\n",
              "0      1          5  2015-07-31   5263        555     1      1            0   \n",
              "1      2          5  2015-07-31   6064        625     1      1            0   \n",
              "2      3          5  2015-07-31   8314        821     1      1            0   \n",
              "3      4          5  2015-07-31  13995       1498     1      1            0   \n",
              "4      5          5  2015-07-31   4822        559     1      1            0   \n",
              "\n",
              "   SchoolHoliday  \n",
              "0              1  \n",
              "1              1  \n",
              "2              1  \n",
              "3              1  \n",
              "4              1  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Missing values (train):\n",
            "Store            0\n",
            "DayOfWeek        0\n",
            "Date             0\n",
            "Sales            0\n",
            "Customers        0\n",
            "Open             0\n",
            "Promo            0\n",
            "StateHoliday     0\n",
            "SchoolHoliday    0\n",
            "dtype: int64\n",
            "\n",
            "Date range:\n",
            "Min date: 2013-01-01\n",
            "Max date: 2015-07-31\n",
            "\n",
            "Unique stores: 1115\n",
            "\n",
            "After merge:\n",
            "Train merged shape: (1017209, 18)\n",
            "Test merged shape : (41088, 17)\n",
            "\n",
            "Sales stats:\n",
            "count    1.017209e+06\n",
            "mean     5.773819e+03\n",
            "std      3.849926e+03\n",
            "min      0.000000e+00\n",
            "25%      3.727000e+03\n",
            "50%      5.744000e+03\n",
            "75%      7.856000e+03\n",
            "max      4.155100e+04\n",
            "Name: Sales, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# ============================\n",
        "# CELL 2: Load Rossmann Data\n",
        "# ============================\n",
        "\n",
        "# Update paths if needed (Colab default: /content/)\n",
        "TRAIN_PATH = \"/content/train.csv\"\n",
        "TEST_PATH  = \"/content/test.csv\"\n",
        "STORE_PATH = \"/content/store.csv\"\n",
        "\n",
        "train_df = pd.read_csv(TRAIN_PATH)\n",
        "test_df  = pd.read_csv(TEST_PATH)\n",
        "store_df = pd.read_csv(STORE_PATH)\n",
        "\n",
        "print(\"Train shape:\", train_df.shape)\n",
        "print(\"Test shape :\", test_df.shape)\n",
        "print(\"Store shape:\", store_df.shape)\n",
        "\n",
        "print(\"\\n--- Train columns ---\")\n",
        "print(train_df.columns.tolist())\n",
        "\n",
        "print(\"\\n--- Sample rows (train) ---\")\n",
        "display(train_df.head())\n",
        "\n",
        "# ----------------------------\n",
        "# Basic sanity checks\n",
        "# ----------------------------\n",
        "print(\"\\nMissing values (train):\")\n",
        "print(train_df.isnull().sum().sort_values(ascending=False).head(10))\n",
        "\n",
        "print(\"\\nDate range:\")\n",
        "print(\"Min date:\", train_df['Date'].min())\n",
        "print(\"Max date:\", train_df['Date'].max())\n",
        "\n",
        "print(\"\\nUnique stores:\", train_df['Store'].nunique())\n",
        "\n",
        "# ----------------------------\n",
        "# Merge store info (no leakage)\n",
        "# ----------------------------\n",
        "train_merged = train_df.merge(store_df, on=\"Store\", how=\"left\")\n",
        "test_merged  = test_df.merge(store_df, on=\"Store\", how=\"left\")\n",
        "\n",
        "print(\"\\nAfter merge:\")\n",
        "print(\"Train merged shape:\", train_merged.shape)\n",
        "print(\"Test merged shape :\", test_merged.shape)\n",
        "\n",
        "# ----------------------------\n",
        "# Target distribution\n",
        "# ----------------------------\n",
        "print(\"\\nSales stats:\")\n",
        "print(train_merged['Sales'].describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dh-iGp5Iovpu",
        "outputId": "730e5f32-3ee2-4169-ec41-9cab6f242f8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train size: (675513, 13)\n",
            "Validation size: (168879, 13)\n",
            "\n",
            "Target stats (log-sales):\n",
            "count    675513.000000\n",
            "mean          8.757269\n",
            "std           0.430584\n",
            "min           0.000000\n",
            "25%           8.488794\n",
            "50%           8.759512\n",
            "75%           9.031333\n",
            "max          10.634701\n",
            "Name: LogSales, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# =============================\n",
        "# CELL 3: Feature Engineering\n",
        "# =============================\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = train_merged.copy()\n",
        "\n",
        "# -----------------------------\n",
        "# Date features\n",
        "# -----------------------------\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df['Year'] = df['Date'].dt.year\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['Day'] = df['Date'].dt.day\n",
        "df['WeekOfYear'] = df['Date'].dt.isocalendar().week.astype(int)\n",
        "\n",
        "# -----------------------------\n",
        "# Categorical cleanup\n",
        "# -----------------------------\n",
        "df['StateHoliday'] = df['StateHoliday'].astype(str)\n",
        "\n",
        "# -----------------------------\n",
        "# Target transform (standard in literature)\n",
        "# -----------------------------\n",
        "df = df[df['Open'] == 1]  # closed stores → zero sales (remove)\n",
        "df['LogSales'] = np.log1p(df['Sales'])\n",
        "\n",
        "# -----------------------------\n",
        "# Feature selection (baseline-safe)\n",
        "# -----------------------------\n",
        "features = [\n",
        "    'Store', 'DayOfWeek', 'Promo', 'Promo2',\n",
        "    'SchoolHoliday', 'Customers',\n",
        "    'Year', 'Month', 'Day', 'WeekOfYear',\n",
        "    'StoreType', 'Assortment', 'CompetitionDistance'\n",
        "]\n",
        "\n",
        "X = df[features]\n",
        "y = df['LogSales']\n",
        "\n",
        "# -----------------------------\n",
        "# Train / validation split (random, standard)\n",
        "# -----------------------------\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Train size:\", X_train.shape)\n",
        "print(\"Validation size:\", X_val.shape)\n",
        "\n",
        "print(\"\\nTarget stats (log-sales):\")\n",
        "print(y_train.describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQLNHap3o_xr",
        "outputId": "c61a9d35-0e7d-4be6-d734-3cf7b121a968"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Linear Regression (Ridge) Results with Imputation\n",
            "RMSE (log-sales): 0.2246\n",
            "MAE  (log-sales): 0.1617\n"
          ]
        }
      ],
      "source": [
        "# =============================\n",
        "# CELL 4: Baseline 1 — Linear Regression (with imputation)\n",
        "# =============================\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# -----------------------------\n",
        "# Feature types\n",
        "# -----------------------------\n",
        "categorical_features = ['StoreType', 'Assortment']\n",
        "numeric_features = [c for c in X_train.columns if c not in categorical_features]\n",
        "\n",
        "# -----------------------------\n",
        "# Preprocessing with imputation\n",
        "# -----------------------------\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Linear model (Ridge)\n",
        "# -----------------------------\n",
        "linreg = Pipeline(steps=[\n",
        "    ('preprocess', preprocessor),\n",
        "    ('model', Ridge(alpha=1.0))\n",
        "])\n",
        "\n",
        "# -----------------------------\n",
        "# Train\n",
        "# -----------------------------\n",
        "linreg.fit(X_train, y_train)\n",
        "\n",
        "# -----------------------------\n",
        "# Evaluate\n",
        "# -----------------------------\n",
        "# -----------------------------\n",
        "# Evaluate\n",
        "# -----------------------------\n",
        "y_pred = linreg.predict(X_val)\n",
        "\n",
        "# RMSE manually\n",
        "rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
        "mae = mean_absolute_error(y_val, y_pred)\n",
        "\n",
        "print(\"Linear Regression (Ridge) Results with Imputation\")\n",
        "print(f\"RMSE (log-sales): {rmse:.4f}\")\n",
        "print(f\"MAE  (log-sales): {mae:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xywwpP6Jq2CX",
        "outputId": "11e81028-6131-422a-fc05-e22004aa4481"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest Results\n",
            "RMSE (log-sales): 0.0919\n",
            "MAE  (log-sales): 0.0695\n"
          ]
        }
      ],
      "source": [
        "# =============================\n",
        "# CELL 5: Baseline 2 — Random Forest\n",
        "# =============================\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# -----------------------------\n",
        "# Random Forest (natively handles nonlinearities)\n",
        "# -----------------------------\n",
        "rf_model = Pipeline(steps=[\n",
        "    ('preprocess', preprocessor),  # same imputation + scaling + one-hot\n",
        "    ('model', RandomForestRegressor(\n",
        "        n_estimators=100,\n",
        "        max_depth=15,\n",
        "        n_jobs=-1,\n",
        "        random_state=42\n",
        "    ))\n",
        "])\n",
        "\n",
        "# -----------------------------\n",
        "# Train\n",
        "# -----------------------------\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# -----------------------------\n",
        "# Evaluate\n",
        "# -----------------------------\n",
        "y_pred_rf = rf_model.predict(X_val)\n",
        "\n",
        "rmse_rf = np.sqrt(mean_squared_error(y_val, y_pred_rf))\n",
        "mae_rf = mean_absolute_error(y_val, y_pred_rf)\n",
        "\n",
        "print(\"Random Forest Results\")\n",
        "print(f\"RMSE (log-sales): {rmse_rf:.4f}\")\n",
        "print(f\"MAE  (log-sales): {mae_rf:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WTVD91UFsEYf",
        "outputId": "353531b3-4c6d-43a0-abf4-70aa0121fb9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029995 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 894\n",
            "[LightGBM] [Info] Number of data points in the train set: 675513, number of used features: 18\n",
            "[LightGBM] [Info] Start training from score 8.757269\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LightGBM Results (Encoded)\n",
            "RMSE (log-sales): 0.0682\n",
            "MAE  (log-sales): 0.0512\n"
          ]
        }
      ],
      "source": [
        "# =============================\n",
        "# CELL 6 (FIXED): LightGBM Baseline with Encoding\n",
        "# =============================\n",
        "\n",
        "import numpy as np\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# -----------------------------\n",
        "# Identify categorical columns\n",
        "# -----------------------------\n",
        "categorical_cols = ['StoreType', 'Assortment']\n",
        "numeric_cols = [c for c in X_train.columns if c not in categorical_cols]\n",
        "\n",
        "# -----------------------------\n",
        "# Preprocessing\n",
        "# -----------------------------\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', 'passthrough', numeric_cols),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# LightGBM model\n",
        "# -----------------------------\n",
        "lgbm_model = LGBMRegressor(\n",
        "    n_estimators=500,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=10,\n",
        "    num_leaves=31,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Pipeline\n",
        "# -----------------------------\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocess', preprocessor),\n",
        "    ('model', lgbm_model)\n",
        "])\n",
        "\n",
        "# -----------------------------\n",
        "# Train\n",
        "# -----------------------------\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# -----------------------------\n",
        "# Predict & Evaluate\n",
        "# -----------------------------\n",
        "y_pred_gbm = pipeline.predict(X_val)\n",
        "\n",
        "rmse_gbm = np.sqrt(mean_squared_error(y_val, y_pred_gbm))\n",
        "mae_gbm = mean_absolute_error(y_val, y_pred_gbm)\n",
        "\n",
        "print(\"LightGBM Results (Encoded)\")\n",
        "print(f\"RMSE (log-sales): {rmse_gbm:.4f}\")\n",
        "print(f\"MAE  (log-sales): {mae_gbm:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VeKyzCAmt0Lv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "838089fa-a032-4fc8-9616-df9d0e9a7293"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3285588649.py:50: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X[numeric_cols] = X[numeric_cols].fillna(X[numeric_cols].median())\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature group sizes:\n",
            "Control: 2 features\n",
            "Context: 4 features\n",
            "Structure: 6 features\n",
            "Epoch [1/30] | Train MSE: 5.1891\n",
            "Epoch [5/30] | Train MSE: 0.0240\n",
            "Epoch [10/30] | Train MSE: 0.0225\n",
            "Epoch [15/30] | Train MSE: 0.0229\n",
            "Epoch [20/30] | Train MSE: 0.0223\n",
            "Epoch [25/30] | Train MSE: 0.0217\n",
            "Epoch [30/30] | Train MSE: 0.0221\n",
            "Hierarchical NN training complete ✅\n",
            "\n",
            "Hierarchical NN Results\n",
            "RMSE (log-sales): 0.1541\n",
            "MAE  (log-sales): 0.1212\n"
          ]
        }
      ],
      "source": [
        "# =============================\n",
        "# CELL: Hierarchical Neural Network (NaN-safe, Rossmann)\n",
        "# =============================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# -----------------------------\n",
        "# 1. FILTER + SAFE TARGET\n",
        "# -----------------------------\n",
        "# Assumes df already exists (train + store merged)\n",
        "\n",
        "df_nn = df.copy()\n",
        "\n",
        "# Remove closed stores and zero sales (CRITICAL)\n",
        "df_nn = df_nn[df_nn[\"Open\"] == 1]\n",
        "df_nn = df_nn[df_nn[\"Sales\"] > 0]\n",
        "\n",
        "# Safe log transform\n",
        "df_nn[\"LogSales\"] = np.log1p(df_nn[\"Sales\"])\n",
        "\n",
        "# -----------------------------\n",
        "# 2. FEATURE ENGINEERING\n",
        "# -----------------------------\n",
        "df_nn[\"Date\"] = pd.to_datetime(df_nn[\"Date\"])\n",
        "df_nn[\"Month\"] = df_nn[\"Date\"].dt.month\n",
        "\n",
        "# Business-aligned features\n",
        "control_features = [\"Promo\", \"Promo2\"]\n",
        "context_features = [\"DayOfWeek\", \"Customers\", \"SchoolHoliday\", \"Month\"]\n",
        "structural_features = [\"CompetitionDistance\", \"StoreType\", \"Assortment\"]\n",
        "\n",
        "features = control_features + context_features + structural_features\n",
        "target = \"LogSales\"\n",
        "\n",
        "X = df_nn[features]\n",
        "y = df_nn[target]\n",
        "\n",
        "# -----------------------------\n",
        "# 3. IMPUTE NUMERICS\n",
        "# -----------------------------\n",
        "numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
        "X[numeric_cols] = X[numeric_cols].fillna(X[numeric_cols].median())\n",
        "\n",
        "# -----------------------------\n",
        "# 4. ONE-HOT ENCODE CATEGORICALS\n",
        "# -----------------------------\n",
        "X = pd.get_dummies(X, columns=[\"StoreType\", \"Assortment\"], drop_first=True)\n",
        "\n",
        "# -----------------------------\n",
        "# 5. TRAIN / VALIDATION SPLIT\n",
        "# -----------------------------\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# 6. SCALE FEATURES\n",
        "# -----------------------------\n",
        "scaler = StandardScaler()\n",
        "X_train = pd.DataFrame(\n",
        "    scaler.fit_transform(X_train),\n",
        "    columns=X_train.columns,\n",
        "    index=X_train.index\n",
        ")\n",
        "X_val = pd.DataFrame(\n",
        "    scaler.transform(X_val),\n",
        "    columns=X_val.columns,\n",
        "    index=X_val.index\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# 7. FEATURE GROUP SLICES\n",
        "# -----------------------------\n",
        "all_features = list(X_train.columns)\n",
        "\n",
        "control_idx = [all_features.index(f) for f in control_features]\n",
        "context_idx = [all_features.index(f) for f in context_features]\n",
        "structure_idx = [\n",
        "    i for i, c in enumerate(all_features)\n",
        "    if c.startswith(\"CompetitionDistance\")\n",
        "    or c.startswith(\"StoreType_\")\n",
        "    or c.startswith(\"Assortment_\")\n",
        "]\n",
        "\n",
        "slices = {\n",
        "    \"Control\": control_idx,\n",
        "    \"Context\": context_idx,\n",
        "    \"Structure\": structure_idx\n",
        "}\n",
        "\n",
        "print(\"Feature group sizes:\")\n",
        "for k, v in slices.items():\n",
        "    print(f\"{k}: {len(v)} features\")\n",
        "\n",
        "# -----------------------------\n",
        "# 8. TENSORS\n",
        "# -----------------------------\n",
        "X_train_t = torch.tensor(X_train.values, dtype=torch.float32)\n",
        "X_val_t   = torch.tensor(X_val.values, dtype=torch.float32)\n",
        "y_train_t = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)\n",
        "y_val_t   = torch.tensor(y_val.values, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# -----------------------------\n",
        "# 9. HIERARCHICAL NN\n",
        "# -----------------------------\n",
        "class HierarchicalNN(nn.Module):\n",
        "    def __init__(self, slices):\n",
        "        super().__init__()\n",
        "        H = 64\n",
        "\n",
        "        self.control_fc = nn.Sequential(\n",
        "            nn.Linear(len(slices[\"Control\"]), H),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.context_fc = nn.Sequential(\n",
        "            nn.Linear(H + len(slices[\"Context\"]), H),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.structure_fc = nn.Sequential(\n",
        "            nn.Linear(H + len(slices[\"Structure\"]), H),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.out = nn.Linear(H, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        c = x[:, slices[\"Control\"]]\n",
        "        ctx = x[:, slices[\"Context\"]]\n",
        "        s = x[:, slices[\"Structure\"]]\n",
        "\n",
        "        h_c = self.control_fc(c)\n",
        "        h_ctx = self.context_fc(torch.cat([h_c, ctx], dim=1))\n",
        "        h_s = self.structure_fc(torch.cat([h_ctx, s], dim=1))\n",
        "\n",
        "        return self.out(h_s)\n",
        "\n",
        "# -----------------------------\n",
        "# 10. TRAINING\n",
        "# -----------------------------\n",
        "model = HierarchicalNN(slices).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    TensorDataset(X_train_t.to(device), y_train_t.to(device)),\n",
        "    batch_size=2048,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "EPOCHS = 30\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for xb, yb in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(xb)\n",
        "        loss = criterion(preds, yb)\n",
        "        loss.backward()\n",
        "\n",
        "        # CRITICAL: gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * xb.size(0)\n",
        "\n",
        "    if epoch % 5 == 0 or epoch == 1:\n",
        "        print(f\"Epoch [{epoch}/{EPOCHS}] | Train MSE: {total_loss / len(train_loader.dataset):.4f}\")\n",
        "\n",
        "print(\"Hierarchical NN training complete ✅\")\n",
        "\n",
        "# -----------------------------\n",
        "# 11. VALIDATION\n",
        "# -----------------------------\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    val_preds = model(X_val_t.to(device)).cpu().numpy()\n",
        "\n",
        "rmse_nn = np.sqrt(mean_squared_error(y_val, val_preds))\n",
        "mae_nn  = mean_absolute_error(y_val, val_preds)\n",
        "\n",
        "print(\"\\nHierarchical NN Results\")\n",
        "print(f\"RMSE (log-sales): {rmse_nn:.4f}\")\n",
        "print(f\"MAE  (log-sales): {mae_nn:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "oV_kkYxTvn5H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "835346ea-8f4b-420d-d252-37fa1fdfd916"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hierarchical NN RMSE: 0.1541\n",
            "Hierarchical NN MAE : 0.1212\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    val_preds = model(X_val_t.to(device)).cpu().numpy()\n",
        "\n",
        "rmse_nn = np.sqrt(mean_squared_error(y_val, val_preds))\n",
        "mae_nn  = mean_absolute_error(y_val, val_preds)\n",
        "\n",
        "print(f\"Hierarchical NN RMSE: {rmse_nn:.4f}\")\n",
        "print(f\"Hierarchical NN MAE : {mae_nn:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "_HyjnJ7jIBS-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a114fce0-65b3-4309-81f1-7298bad13305"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pytensor 2.36.3 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "rasterio 1.5.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "tobler 0.13.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mShapes ready | Context: torch.Size([4800, 4]) Control: torch.Size([4800, 6]) Structure: torch.Size([4800, 3])\n",
            "Graph ready: nodes = 3 edges = 6\n",
            "Epoch [1/50] | Train MSE: 16896.9382\n",
            "Epoch [10/50] | Train MSE: 134.5263\n",
            "Epoch [20/50] | Train MSE: 85.6694\n",
            "Epoch [30/50] | Train MSE: 75.5636\n",
            "Epoch [40/50] | Train MSE: 70.2749\n",
            "Epoch [50/50] | Train MSE: 64.1185\n",
            "GNN Training complete ✅\n",
            "\n",
            "--- BUSINESS-ALIGNED ATTRIBUTION ---\n",
            "Context (non-actionable)   : 19.92 (29.9%)\n",
            "Control (business levers)  : 24.12 (36.1%)\n",
            "Structure (outlet effects) : 22.69 (34.0%)\n"
          ]
        }
      ],
      "source": [
        "# =========================================\n",
        "# FINAL CELL: Business-Aware GNN + Captum Explainability\n",
        "# =========================================\n",
        "\n",
        "!pip install torch-geometric torch-scatter torch-sparse torch-cluster torch-spline-conv -q\n",
        "!pip install captum -q\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.data import Data\n",
        "from captum.attr import IntegratedGradients\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# -----------------------------\n",
        "# 0. Seed + device\n",
        "# -----------------------------\n",
        "RANDOM_SEED = 42\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Simulated business data\n",
        "# -----------------------------\n",
        "def generate_business_data(n_samples=6000):\n",
        "    rng = np.random.RandomState(RANDOM_SEED)\n",
        "    month = rng.randint(1,13,n_samples)\n",
        "    seasonality = 12 * np.sin(month*np.pi/6)\n",
        "    sales_lag1 = rng.normal(100,25,n_samples).clip(20)\n",
        "    cannibal_sales_lag1 = rng.normal(60,18,n_samples).clip(10)\n",
        "    outlet_type = rng.choice([\"Premium\",\"Standard\",\"Value\"], n_samples, p=[0.25,0.55,0.20])\n",
        "    brand_equity = rng.uniform(0.7,1.0,n_samples)\n",
        "    ad_spend = rng.choice([0,500,1000,2000], n_samples, p=[0.45,0.30,0.15,0.10])\n",
        "    promo_type = rng.choice([\"None\",\"Discount\",\"Bundle\",\"Slab\"], n_samples, p=[0.6,0.2,0.1,0.1])\n",
        "    discount_pct = np.where(promo_type==\"Discount\", rng.uniform(0.1,0.35,n_samples), 0.0)\n",
        "    base_map = {\"Premium\":95,\"Standard\":65,\"Value\":45}\n",
        "    base_demand = np.vectorize(base_map.get)(outlet_type)\n",
        "    momentum = 0.45*sales_lag1\n",
        "    cannibal_drag = -0.18*cannibal_sales_lag1\n",
        "    roas_map = {0:0,500:6,1000:14,2000:24}\n",
        "    ad_lift = np.vectorize(roas_map.get)(ad_spend)\n",
        "    promo_lift = np.zeros(n_samples)\n",
        "    promo_lift += np.where(promo_type==\"Slab\",22,0)\n",
        "    promo_lift += np.where(promo_type==\"Bundle\",15,0)\n",
        "    promo_lift += np.where(promo_type==\"Discount\",12+90*discount_pct,0)\n",
        "    brand_halo = 12*brand_equity\n",
        "    noise = rng.normal(0,6,n_samples)\n",
        "    units_sold = (base_demand + momentum + seasonality + ad_lift + promo_lift + cannibal_drag + brand_halo + noise).clip(5)\n",
        "    df = pd.DataFrame({\n",
        "        \"month\":month, \"sales_lag1\":sales_lag1, \"cannibal_sales_lag1\":cannibal_sales_lag1,\n",
        "        \"outlet_type\":outlet_type, \"brand_equity\":brand_equity,\n",
        "        \"ad_spend\":ad_spend, \"promo_type\":promo_type, \"discount_pct\":discount_pct,\n",
        "        \"units_sold\":units_sold\n",
        "    })\n",
        "    return df\n",
        "\n",
        "df = generate_business_data(6000)\n",
        "\n",
        "# -----------------------------\n",
        "# 2. Features & encoding\n",
        "# -----------------------------\n",
        "y = torch.tensor(df[\"units_sold\"].values, dtype=torch.float32).view(-1,1)\n",
        "\n",
        "# Contextual\n",
        "context_features = [\"month\",\"sales_lag1\",\"cannibal_sales_lag1\",\"brand_equity\"]\n",
        "scaler_ctx = StandardScaler()\n",
        "Xc = torch.tensor(scaler_ctx.fit_transform(df[context_features]), dtype=torch.float32)\n",
        "\n",
        "# Control\n",
        "control_numeric = [\"ad_spend\",\"discount_pct\"]\n",
        "scaler_ctl = StandardScaler()\n",
        "Xctl_num = scaler_ctl.fit_transform(df[control_numeric])\n",
        "\n",
        "control_categorical = [\"promo_type\"]\n",
        "enc_promo = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
        "Xctl_cat = enc_promo.fit_transform(df[control_categorical])\n",
        "\n",
        "Xctl = torch.tensor(np.concatenate([Xctl_num, Xctl_cat], axis=1), dtype=torch.float32)\n",
        "\n",
        "# Structural\n",
        "structural_features = [\"outlet_type\"]\n",
        "enc_outlet = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
        "Xs = torch.tensor(enc_outlet.fit_transform(df[structural_features]), dtype=torch.float32)\n",
        "\n",
        "# -----------------------------\n",
        "# 3. Train-test split\n",
        "# -----------------------------\n",
        "Xc_train, Xc_test, Xctl_train, Xctl_test, Xs_train, Xs_test, y_train, y_test = train_test_split(\n",
        "    Xc, Xctl, Xs, y, test_size=0.2, random_state=RANDOM_SEED\n",
        ")\n",
        "\n",
        "print(\"Shapes ready | Context:\", Xc_train.shape, \"Control:\", Xctl_train.shape, \"Structure:\", Xs_train.shape)\n",
        "\n",
        "# -----------------------------\n",
        "# 4. Graph for structural GCN\n",
        "# -----------------------------\n",
        "num_nodes = Xs.shape[1]\n",
        "edge_index = []\n",
        "for i in range(num_nodes):\n",
        "    for j in range(num_nodes):\n",
        "        if i!=j:\n",
        "            edge_index.append([i,j])\n",
        "edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
        "print(f\"Graph ready: nodes = {num_nodes} edges = {edge_index.shape[1]}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 5. GNN Model\n",
        "# -----------------------------\n",
        "class BusinessAwareGNN(nn.Module):\n",
        "    def __init__(self, ctx_dim, ctl_dim, str_dim):\n",
        "        super().__init__()\n",
        "        H = 32\n",
        "        self.context_net = nn.Sequential(nn.Linear(ctx_dim,H), nn.ReLU())\n",
        "        self.control_net = nn.Sequential(nn.Linear(ctl_dim,H), nn.ReLU())\n",
        "        self.structural_gcn = GCNConv(str_dim,H)\n",
        "        self.fusion = nn.Sequential(nn.Linear(H*3,H), nn.ReLU(), nn.Linear(H,1))\n",
        "\n",
        "    def forward(self, x_ctx, x_ctl, x_str):\n",
        "        h_ctx = self.context_net(x_ctx)\n",
        "        h_ctl = self.control_net(x_ctl)\n",
        "        h_str = torch.relu(self.structural_gcn(x_str, edge_index.to(x_str.device)))\n",
        "        return self.fusion(torch.cat([h_ctx,h_ctl,h_str], dim=1))\n",
        "\n",
        "model_gnn = BusinessAwareGNN(Xc_train.shape[1], Xctl_train.shape[1], Xs_train.shape[1]).to(device)\n",
        "optimizer = torch.optim.Adam(model_gnn.parameters(), lr=0.001)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# -----------------------------\n",
        "# 6. Training loop\n",
        "# -----------------------------\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 64\n",
        "dataset = torch.utils.data.TensorDataset(Xc_train, Xctl_train, Xs_train, y_train)\n",
        "loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model_gnn.train()\n",
        "    total_loss = 0\n",
        "    for xc, xctl, xs, yb in loader:\n",
        "        xc, xctl, xs, yb = xc.to(device), xctl.to(device), xs.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        preds = model_gnn(xc, xctl, xs)\n",
        "        loss = criterion(preds, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * yb.size(0)\n",
        "    if (epoch+1)%10==0 or epoch==0:\n",
        "        print(f\"Epoch [{epoch+1}/{EPOCHS}] | Train MSE: {total_loss/len(dataset):.4f}\")\n",
        "\n",
        "print(\"GNN Training complete ✅\")\n",
        "\n",
        "# -----------------------------\n",
        "# 7. Captum Integrated Gradients\n",
        "# -----------------------------\n",
        "model_gnn.eval()\n",
        "ig = IntegratedGradients(model_gnn.forward)\n",
        "\n",
        "baseline_ctx = torch.zeros_like(Xc_test[:1]).to(device)\n",
        "baseline_ctl = torch.zeros_like(Xctl_test[:1]).to(device)\n",
        "baseline_str = torch.zeros_like(Xs_test[:1]).to(device)\n",
        "\n",
        "inputs = (Xc_test[:300].to(device), Xctl_test[:300].to(device), Xs_test[:300].to(device))\n",
        "baselines = (baseline_ctx, baseline_ctl, baseline_str)\n",
        "\n",
        "attr_ctx, attr_ctl, attr_str = ig.attribute(\n",
        "    inputs=inputs,\n",
        "    baselines=baselines,\n",
        "    n_steps=50\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# 8. Aggregate attribution\n",
        "# -----------------------------\n",
        "ctx_contrib = np.mean(np.sum(np.abs(attr_ctx.cpu().detach().numpy()),axis=1))\n",
        "ctl_contrib = np.mean(np.sum(np.abs(attr_ctl.cpu().detach().numpy()),axis=1))\n",
        "str_contrib = np.mean(np.sum(np.abs(attr_str.cpu().detach().numpy()),axis=1))\n",
        "total = ctx_contrib + ctl_contrib + str_contrib\n",
        "\n",
        "print(\"\\n--- BUSINESS-ALIGNED ATTRIBUTION ---\")\n",
        "print(f\"Context (non-actionable)   : {ctx_contrib:.2f} ({ctx_contrib/total:.1%})\")\n",
        "print(f\"Control (business levers)  : {ctl_contrib:.2f} ({ctl_contrib/total:.1%})\")\n",
        "print(f\"Structure (outlet effects) : {str_contrib:.2f} ({str_contrib/total:.1%})\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================\n",
        "# FINAL SINGLE CELL: TRAIN + EVAL + ATTRIBUTION\n",
        "# =============================================\n",
        "\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "import numpy as np\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.data import Data\n",
        "from captum.attr import IntegratedGradients\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "RANDOM_SEED = 42\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "# ----------------------------\n",
        "# Assume Xc_train/Xctl_train/Xs_train/y_train\n",
        "# and Xc_test/Xctl_test/Xs_test/y_test, graph_data\n",
        "# are already defined\n",
        "# ----------------------------\n",
        "\n",
        "# ----------------------------\n",
        "# 1. Define GNN Model\n",
        "# ----------------------------\n",
        "H = 32\n",
        "num_nodes = Xs_train.shape[1]  # number of structural features\n",
        "\n",
        "class StructuralGCN(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.gcn = GCNConv(in_dim, out_dim)\n",
        "    def forward(self, x, edge_index):\n",
        "        return torch.relu(self.gcn(x, edge_index))\n",
        "\n",
        "class BusinessAwareGNN(nn.Module):\n",
        "    def __init__(self, ctx_dim, ctl_dim, str_dim):\n",
        "        super().__init__()\n",
        "        self.context_net = nn.Sequential(nn.Linear(ctx_dim,H), nn.ReLU())\n",
        "        self.control_net = nn.Sequential(nn.Linear(ctl_dim,H), nn.ReLU())\n",
        "        self.structural_gcn = StructuralGCN(str_dim,H)\n",
        "        self.fusion = nn.Sequential(nn.Linear(H*3,H), nn.ReLU(), nn.Linear(H,1))\n",
        "        # TEMP fusion for IG (context + control only)\n",
        "        self.fusion_ig = nn.Sequential(nn.Linear(H*2,H), nn.ReLU(), nn.Linear(H,1))\n",
        "    def forward(self, x_ctx, x_ctl, x_str, graph):\n",
        "        h_ctx = self.context_net(x_ctx)\n",
        "        h_ctl = self.control_net(x_ctl)\n",
        "        g_emb = self.structural_gcn(graph.x, graph.edge_index)\n",
        "        node_ids = torch.argmax(x_str, dim=1)\n",
        "        h_str = g_emb[node_ids]\n",
        "        return self.fusion(torch.cat([h_ctx,h_ctl,h_str], dim=1))\n",
        "    def forward_ig(self, x_ctx, x_ctl):\n",
        "        h_ctx = self.context_net(x_ctx)\n",
        "        h_ctl = self.control_net(x_ctl)\n",
        "        return self.fusion_ig(torch.cat([h_ctx,h_ctl], dim=1))\n",
        "\n",
        "# ----------------------------\n",
        "# 2. Initialize & Train\n",
        "# ----------------------------\n",
        "model_gnn = BusinessAwareGNN(Xc_train.shape[1], Xctl_train.shape[1], num_nodes).to(device)\n",
        "optimizer = optim.Adam(model_gnn.parameters(), lr=0.001)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 64\n",
        "train_dataset = torch.utils.data.TensorDataset(\n",
        "    Xc_train.to(device), Xctl_train.to(device), Xs_train.to(device), y_train.to(device)\n",
        ")\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "model_gnn.train()\n",
        "for epoch in range(EPOCHS):\n",
        "    total_loss = 0\n",
        "    for xc,xctl,xs,y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        preds = model_gnn(xc,xctl,xs,graph_data)\n",
        "        loss = criterion(preds,y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * y.size(0)\n",
        "    if (epoch+1)%10==0:\n",
        "        print(f\"Epoch [{epoch+1}/{EPOCHS}] | Train MSE: {total_loss/len(train_dataset):.4f}\")\n",
        "print(\"GNN Training complete ✅\")\n",
        "\n",
        "# ----------------------------\n",
        "# 3. Evaluate\n",
        "# ----------------------------\n",
        "model_gnn.eval()\n",
        "with torch.no_grad():\n",
        "    preds_test = model_gnn(Xc_test.to(device), Xctl_test.to(device), Xs_test.to(device), graph_data)\n",
        "rmse = torch.sqrt(torch.mean((preds_test - y_test.to(device))**2))\n",
        "mae = torch.mean(torch.abs(preds_test - y_test.to(device)))\n",
        "print(f\"GNN RMSE: {rmse.item():.4f} | MAE: {mae.item():.4f}\")\n",
        "\n",
        "# ----------------------------\n",
        "# 4. Integrated Gradients (context + control only)\n",
        "# ----------------------------\n",
        "N_EXPLAIN = 300\n",
        "inputs_cc = torch.cat([Xc_test[:N_EXPLAIN], Xctl_test[:N_EXPLAIN]], dim=1).to(device)\n",
        "baseline_cc = torch.zeros_like(inputs_cc).to(device)\n",
        "\n",
        "def forward_cc(x):\n",
        "    xc = x[:, :Xc_test.shape[1]]\n",
        "    xctl = x[:, Xc_test.shape[1]:]\n",
        "    return model_gnn.forward_ig(xc,xctl)\n",
        "\n",
        "ig = IntegratedGradients(forward_cc)\n",
        "attr = ig.attribute(inputs_cc, baselines=baseline_cc, n_steps=50)\n",
        "\n",
        "attr_ctx = attr[:, :Xc_test.shape[1]].detach().cpu().numpy()\n",
        "attr_ctl = attr[:, Xc_test.shape[1]:].detach().cpu().numpy()\n",
        "\n",
        "# ----------------------------\n",
        "# 5. Structural contribution (post-hoc)\n",
        "# ----------------------------\n",
        "with torch.no_grad():\n",
        "    g_emb = model_gnn.structural_gcn(graph_data.x, graph_data.edge_index)\n",
        "    node_ids = torch.argmax(Xs_test[:N_EXPLAIN], dim=1)\n",
        "    attr_str = g_emb[node_ids].detach().cpu().numpy()\n",
        "\n",
        "# ----------------------------\n",
        "# 6. Aggregate\n",
        "# ----------------------------\n",
        "context_contribution = np.mean(np.sum(np.abs(attr_ctx), axis=1))\n",
        "control_contribution = np.mean(np.sum(np.abs(attr_ctl), axis=1))\n",
        "structural_contribution = np.mean(np.linalg.norm(attr_str, axis=1))\n",
        "total = context_contribution + control_contribution + structural_contribution\n",
        "\n",
        "print(\"\\n--- BUSINESS-ALIGNED ATTRIBUTION ---\")\n",
        "print(f\"Context   : {context_contribution:.2f} ({context_contribution/total:.1%})\")\n",
        "print(f\"Control   : {control_contribution:.2f} ({control_contribution/total:.1%})\")\n",
        "print(f\"Structure : {structural_contribution:.2f} ({structural_contribution/total:.1%})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVQ1vP6fhDj5",
        "outputId": "82f17ed3-6cd9-41aa-fce8-8e0d97bfddbc"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/50] | Train MSE: 409.9723\n",
            "Epoch [20/50] | Train MSE: 365.1341\n",
            "Epoch [30/50] | Train MSE: 348.0713\n",
            "Epoch [40/50] | Train MSE: 340.4007\n",
            "Epoch [50/50] | Train MSE: 337.1768\n",
            "GNN Training complete ✅\n",
            "GNN RMSE: 18.3732 | MAE: 14.5301\n",
            "\n",
            "--- BUSINESS-ALIGNED ATTRIBUTION ---\n",
            "Context   : 0.09 (2.3%)\n",
            "Control   : 0.21 (5.5%)\n",
            "Structure : 3.53 (92.2%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CS9C3IkTuON"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}